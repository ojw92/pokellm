{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PokeMBTI-LLM\n"
      ],
      "metadata": {
        "id": "7NNwoTQU42bN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change to correct working directory in Colab-Jupyter\n",
        "import os\n",
        "os.chdir('/tf-ojw/pokellm')\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "_LLBDFip41qx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d359a1-d4dd-4143-ef2f-a03eaa09c08f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/tf-ojw/pokellm'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter05/Chapter%205%20-%20Text%20Clustering%20and%20Topic%20Modeling.ipynb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Create an embedding for each abstract\n",
        "embedding_model = SentenceTransformer('thenlper/gte-small')\n",
        "embeddings = embedding_model.encode(abstracts, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "2hQvsNqU4FZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the dimensions of the resulting embeddings\n",
        "embeddings.shape"
      ],
      "metadata": {
        "id": "BUAQvHNd4FVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from umap import UMAP\n",
        "\n",
        "# Reduce the dimensionality of input embeddings from 384 dimenions to 5 dimenions\n",
        "umap_model = UMAP(\n",
        "    n_components=5, min_dist=0.0, metric='cosine', random_state=42\n",
        ")\n",
        "reduced_embeddings = umap_model.fit_transform(embeddings)\n"
      ],
      "metadata": {
        "id": "boqDBDpQ5Trx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluster the reduced embeddings\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "# Fit the model and extract the clusters\n",
        "hdbscan_model = HDBSCAN(\n",
        "    min_cluster_size=50, metric='euclidean', cluster_selection_method='eom'\n",
        ").fit(reduced_embeddings)\n",
        "clusters = hdbscan_model.labels_\n",
        "\n",
        "# How many clusters were generated?\n",
        "len(set(clusters))\n"
      ],
      "metadata": {
        "id": "InABMr2e5Tn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Print first three documents in cluster 0\n",
        "cluster = 0\n",
        "for index in np.where(clusters==cluster)[0][:3]:\n",
        "    print(abstracts[index][:300] + \"... \\n\")\n"
      ],
      "metadata": {
        "id": "rKDNOb_X5TkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Reduce 384-dimensional embeddings to 2 dimensions for easier visualization\n",
        "reduced_embeddings = UMAP(\n",
        "    n_components=2, min_dist=0.0, metric='cosine', random_state=42\n",
        ").fit_transform(embeddings)\n",
        "\n",
        "# Create dataframe\n",
        "df = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"])\n",
        "df[\"title\"] = titles\n",
        "df[\"cluster\"] = [str(c) for c in clusters]\n",
        "\n",
        "# Select outliers and non-outliers (clusters)\n",
        "clusters_df = df.loc[df.cluster != \"-1\", :]\n",
        "outliers_df = df.loc[df.cluster == \"-1\", :]"
      ],
      "metadata": {
        "id": "QPwYWuFm5Tgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot outliers and non-outliers seperately\n",
        "plt.scatter(outliers_df.x, outliers_df.y, alpha=0.05, s=2, c=\"grey\")\n",
        "plt.scatter(\n",
        "    clusters_df.x, clusters_df.y, c=clusters_df.cluster.astype(int),\n",
        "    alpha=0.6, s=2, cmap='tab20b'\n",
        ")\n",
        "plt.axis('off')\n",
        "# plt.savefig(\"matplotlib.png\", dpi=300)  # Uncomment to save the graph as a .png"
      ],
      "metadata": {
        "id": "Ltq-ymWp5Tb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yRLEs1LG5TYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8A0SEgkQ5TUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yXH_8G-K5TQW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip list"
      ],
      "metadata": {
        "id": "lsKSOGZsKJON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e71131b-84b0-4148-df4c-176105c15284"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                      Version\r\n",
            "---------------------------- --------------------\r\n",
            "absl-py                      1.4.0\r\n",
            "accelerate                   0.22.0\r\n",
            "aiohappyeyeballs             2.4.4\r\n",
            "aiohttp                      3.10.11\r\n",
            "aiosignal                    1.3.1\r\n",
            "anyio                        4.0.0\r\n",
            "argon2-cffi                  23.1.0\r\n",
            "argon2-cffi-bindings         21.2.0\r\n",
            "arrow                        1.2.3\r\n",
            "asttokens                    2.4.0\r\n",
            "astunparse                   1.6.3\r\n",
            "async-lru                    2.0.4\r\n",
            "async-timeout                5.0.1\r\n",
            "attrs                        23.1.0\r\n",
            "Babel                        2.12.1\r\n",
            "backcall                     0.2.0\r\n",
            "beautifulsoup4               4.12.2\r\n",
            "bleach                       6.0.0\r\n",
            "cachetools                   5.3.1\r\n",
            "certifi                      2019.11.28\r\n",
            "cffi                         1.15.1\r\n",
            "chardet                      3.0.4\r\n",
            "charset-normalizer           3.2.0\r\n",
            "click                        8.1.8\r\n",
            "cmake                        3.27.4.1\r\n",
            "comm                         0.1.4\r\n",
            "contourpy                    1.1.0\r\n",
            "cycler                       0.11.0\r\n",
            "datasets                     3.1.0\r\n",
            "dbus-python                  1.2.16\r\n",
            "debugpy                      1.6.7.post1\r\n",
            "decorator                    5.1.1\r\n",
            "defusedxml                   0.7.1\r\n",
            "dill                         0.3.8\r\n",
            "exceptiongroup               1.1.3\r\n",
            "executing                    1.2.0\r\n",
            "fastjsonschema               2.18.0\r\n",
            "filelock                     3.12.3\r\n",
            "flatbuffers                  23.5.26\r\n",
            "fonttools                    4.42.1\r\n",
            "fqdn                         1.5.1\r\n",
            "frozenlist                   1.5.0\r\n",
            "fsspec                       2023.9.0\r\n",
            "gast                         0.4.0\r\n",
            "google-auth                  2.21.0\r\n",
            "google-auth-oauthlib         1.0.0\r\n",
            "google-pasta                 0.2.0\r\n",
            "grpcio                       1.56.0\r\n",
            "h5py                         3.9.0\r\n",
            "huggingface-hub              0.25.0\r\n",
            "idna                         2.8\r\n",
            "importlib-metadata           6.7.0\r\n",
            "importlib-resources          6.0.1\r\n",
            "ipykernel                    6.25.2\r\n",
            "ipython                      8.12.2\r\n",
            "isoduration                  20.11.0\r\n",
            "jedi                         0.19.0\r\n",
            "Jinja2                       3.1.2\r\n",
            "joblib                       1.4.2\r\n",
            "json5                        0.9.14\r\n",
            "jsonpointer                  2.4\r\n",
            "jsonschema                   4.19.0\r\n",
            "jsonschema-specifications    2023.7.1\r\n",
            "jupyter_client               8.3.1\r\n",
            "jupyter_core                 5.3.1\r\n",
            "jupyter-events               0.7.0\r\n",
            "jupyter-lsp                  2.2.0\r\n",
            "jupyter_server               2.7.3\r\n",
            "jupyter_server_terminals     0.4.4\r\n",
            "jupyterlab                   4.0.5\r\n",
            "jupyterlab-pygments          0.2.2\r\n",
            "jupyterlab_server            2.24.0\r\n",
            "keras                        2.13.1\r\n",
            "kiwisolver                   1.4.5\r\n",
            "libclang                     16.0.0\r\n",
            "lit                          16.0.6\r\n",
            "Markdown                     3.4.3\r\n",
            "MarkupSafe                   2.1.3\r\n",
            "matplotlib                   3.7.2\r\n",
            "matplotlib-inline            0.1.6\r\n",
            "mistune                      3.0.1\r\n",
            "mpmath                       1.3.0\r\n",
            "multidict                    6.1.0\r\n",
            "multiprocess                 0.70.16\r\n",
            "nbclient                     0.8.0\r\n",
            "nbconvert                    7.8.0\r\n",
            "nbformat                     5.9.2\r\n",
            "nest-asyncio                 1.5.7\r\n",
            "networkx                     3.1\r\n",
            "nltk                         3.9.1\r\n",
            "notebook_shim                0.2.3\r\n",
            "numpy                        1.24.3\r\n",
            "nvidia-cublas-cu11           11.10.3.66\r\n",
            "nvidia-cublas-cu12           12.1.3.1\r\n",
            "nvidia-cuda-cupti-cu11       11.7.101\r\n",
            "nvidia-cuda-cupti-cu12       12.1.105\r\n",
            "nvidia-cuda-nvrtc-cu11       11.7.99\r\n",
            "nvidia-cuda-nvrtc-cu12       12.1.105\r\n",
            "nvidia-cuda-runtime-cu11     11.7.99\r\n",
            "nvidia-cuda-runtime-cu12     12.1.105\r\n",
            "nvidia-cudnn-cu11            8.5.0.96\r\n",
            "nvidia-cudnn-cu12            9.1.0.70\r\n",
            "nvidia-cufft-cu11            10.9.0.58\r\n",
            "nvidia-cufft-cu12            11.0.2.54\r\n",
            "nvidia-curand-cu11           10.2.10.91\r\n",
            "nvidia-curand-cu12           10.3.2.106\r\n",
            "nvidia-cusolver-cu11         11.4.0.1\r\n",
            "nvidia-cusolver-cu12         11.4.5.107\r\n",
            "nvidia-cusparse-cu11         11.7.4.91\r\n",
            "nvidia-cusparse-cu12         12.1.0.106\r\n",
            "nvidia-nccl-cu11             2.14.3\r\n",
            "nvidia-nccl-cu12             2.20.5\r\n",
            "nvidia-nvjitlink-cu12        12.9.41\r\n",
            "nvidia-nvtx-cu11             11.7.91\r\n",
            "nvidia-nvtx-cu12             12.1.105\r\n",
            "oauthlib                     3.2.2\r\n",
            "opt-einsum                   3.3.0\r\n",
            "overrides                    7.4.0\r\n",
            "packaging                    23.1\r\n",
            "pandas                       2.0.3\r\n",
            "pandocfilters                1.5.0\r\n",
            "parso                        0.8.3\r\n",
            "pexpect                      4.8.0\r\n",
            "pickleshare                  0.7.5\r\n",
            "Pillow                       10.0.0\r\n",
            "pip                          23.1.2\r\n",
            "pkgutil_resolve_name         1.3.10\r\n",
            "platformdirs                 3.10.0\r\n",
            "prometheus-client            0.17.1\r\n",
            "prompt-toolkit               3.0.39\r\n",
            "propcache                    0.2.0\r\n",
            "protobuf                     4.23.3\r\n",
            "psutil                       5.9.5\r\n",
            "ptyprocess                   0.7.0\r\n",
            "pure-eval                    0.2.2\r\n",
            "pyarrow                      17.0.0\r\n",
            "pyasn1                       0.5.0\r\n",
            "pyasn1-modules               0.3.0\r\n",
            "pycparser                    2.21\r\n",
            "Pygments                     2.16.1\r\n",
            "PyGObject                    3.36.0\r\n",
            "pyparsing                    3.0.9\r\n",
            "python-apt                   2.0.1+ubuntu0.20.4.1\r\n",
            "python-dateutil              2.8.2\r\n",
            "python-json-logger           2.0.7\r\n",
            "pytz                         2023.3.post1\r\n",
            "PyYAML                       6.0.1\r\n",
            "pyzmq                        25.1.1\r\n",
            "referencing                  0.30.2\r\n",
            "regex                        2023.8.8\r\n",
            "requests                     2.32.3\r\n",
            "requests-oauthlib            1.3.1\r\n",
            "requests-unixsocket          0.2.0\r\n",
            "rfc3339-validator            0.1.4\r\n",
            "rfc3986-validator            0.1.1\r\n",
            "rpds-py                      0.10.2\r\n",
            "rsa                          4.9\r\n",
            "safetensors                  0.5.3\r\n",
            "scikit-learn                 1.3.2\r\n",
            "scipy                        1.10.1\r\n",
            "Send2Trash                   1.8.2\r\n",
            "sentence-transformers        2.2.2\r\n",
            "sentencepiece                0.2.0\r\n",
            "setuptools                   68.0.0\r\n",
            "six                          1.14.0\r\n",
            "sniffio                      1.3.0\r\n",
            "soupsieve                    2.5\r\n",
            "stack-data                   0.6.2\r\n",
            "sympy                        1.12\r\n",
            "tensorboard                  2.13.0\r\n",
            "tensorboard-data-server      0.7.1\r\n",
            "tensorflow                   2.13.0\r\n",
            "tensorflow-estimator         2.13.0\r\n",
            "tensorflow-io-gcs-filesystem 0.32.0\r\n",
            "termcolor                    2.3.0\r\n",
            "terminado                    0.17.1\r\n",
            "threadpoolctl                3.5.0\r\n",
            "tinycss2                     1.2.1\r\n",
            "tokenizers                   0.20.3\r\n",
            "tomli                        2.0.1\r\n",
            "torch                        2.4.1\r\n",
            "torchvision                  0.19.1\r\n",
            "tornado                      6.3.3\r\n",
            "tqdm                         4.67.1\r\n",
            "traitlets                    5.9.0\r\n",
            "transformers                 4.46.3\r\n",
            "triton                       3.0.0\r\n",
            "typing_extensions            4.13.2\r\n",
            "tzdata                       2023.3\r\n",
            "uri-template                 1.3.0\r\n",
            "urllib3                      1.25.8\r\n",
            "wcwidth                      0.2.6\r\n",
            "webcolors                    1.13\r\n",
            "webencodings                 0.5.1\r\n",
            "websocket-client             1.6.2\r\n",
            "Werkzeug                     2.3.6\r\n",
            "wheel                        0.40.0\r\n",
            "wrapt                        1.15.0\r\n",
            "xxhash                       3.5.0\r\n",
            "yarl                         1.15.2\r\n",
            "zipp                         3.15.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install sentence-transformers==2.4.0            # 2.4.0 to run 'intfloat/multilingual-e5-large-instruct' # 2.2.2 to avoid using 'datasets'\n",
        "# pip install huggingface_hub==0.25.0                 # originally  0.30.2\n",
        "# pip install datasets      # latest versions of sentence-transformers use datasets.Dataset under the hood to wrap and iterate over training data\n",
        "\n",
        "# to use 'intfloat/multilingual-e5-large-instruct':\n",
        " \"sentence_transformers\": \"2.4.0.dev0\",\n",
        "    \"transformers\": \"4.37.0\",\n",
        "    \"pytorch\": \"2.1.0+cu121\""
      ],
      "metadata": {
        "id": "0Rm5M4s_KJKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete this portion and keep just the custom similarity pairs\n",
        "\"\"\"\n",
        "from sentence_transformers import InputExample, losses\n",
        "\n",
        "train_examples = [InputExample(texts=[u, p], label=score) for u, p, score in training_data]\n",
        "train_dataset = SentencesDataset(train_examples, model.get_tokenizer())\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
        "\n",
        "train_loss = losses.CosineSimilarityLoss(model)\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3)\n",
        "\"\"\"\n",
        "# Result: Model learns subjective tone-to-description alignment that cosine alone cannot achieve.\n",
        "\n",
        "\n",
        "# custom similarity pairs\n",
        "\n",
        "# Calm / Nature / Kind\n",
        "InputExample(\n",
        "    texts=[\n",
        "        \"I’m introverted and enjoy quiet walks through nature. I’m dependable and grounded.\",\n",
        "        \"Bulbasaur is a calm and grounded Pokémon who enjoys nature and steady growth.\"\n",
        "    ],\n",
        "    label=0.95\n",
        "),\n",
        "\n",
        "# Firey / Brave / Independent\n",
        "InputExample(\n",
        "    texts=[\n",
        "        \"I love challenges, I’m intense and passionate about what I do.\",\n",
        "        \"Charizard is confident, powerful, and breathes fire with great pride.\"\n",
        "    ],\n",
        "    label=0.95\n",
        "),\n",
        "\n",
        "# Sleepy / Easy-going\n",
        "InputExample(\n",
        "    texts=[\n",
        "        \"I like to take things easy and avoid conflict. I’d rather nap than argue.\",\n",
        "        \"Snorlax is a sleepy giant that is chill, relaxed, and rarely gets bothered.\"\n",
        "    ],\n",
        "    label=1.0\n",
        "),\n",
        "\n",
        "# Adaptive / Curious\n",
        "InputExample(\n",
        "    texts=[\n",
        "        \"I’m flexible and like trying different things. I go with the flow.\",\n",
        "        \"Eevee is an adaptable Pokémon with many potential forms.\"\n",
        "    ],\n",
        "    label=0.9\n",
        "),\n",
        "\n",
        "# Playful / Energetic\n",
        "InputExample(\n",
        "    texts=[\n",
        "        \"I’m energetic, social, and love being around friends.\",\n",
        "        \"Pikachu is an energetic and friendly electric mouse known for loyalty.\"\n",
        "    ],\n",
        "    label=0.95\n",
        "),\n",
        "\n",
        "# Negative / Mismatched Pairs (Important for contrast)\n",
        "InputExample(\n",
        "    texts=[\n",
        "        \"I’m slow-paced and calm. I dislike being rushed.\",\n",
        "        \"Charizard is aggressive and proud, breathing fire to overwhelm opponents.\"\n",
        "    ],\n",
        "    label=0.2\n",
        "),\n",
        "InputExample(\n",
        "    texts=[\n",
        "        \"I’m quiet and introverted.\",\n",
        "        \"Pikachu is loud, playful, and energetic.\"\n",
        "    ],\n",
        "    label=0.3\n",
        "),\n",
        "InputExample(\n",
        "    texts=[\n",
        "        \"I enjoy peaceful meditation and spiritual thought.\",\n",
        "        \"Machamp is a physical fighter who punches mountains for training.\"\n",
        "    ],\n",
        "    label=0.1\n",
        "),\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KxwdD5EwKJGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util, InputExample, losses, SentencesDataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "# from torch.nn.functional import cosine_similarity\n",
        "# from datasets import Dataset\n",
        "\n",
        "\n",
        "# Load model\n",
        "#MODEL_NAME = 'all-MiniLM-L6-v2'   # 'all-MiniLM-L6-v2' is great for general similarity (news, reviews), but not optimized for abstract personality-tone alignment\n",
        "#MODEL_NAME = 'all-mpnet-base-v2'   # 'all-mpnet-base-v2' is larger, more nuanced, better with sentence-level semantics\n",
        "MODEL_NAME = 'intfloat/multilingual-e5-large-instruct'\n",
        "\n",
        "\"\"\"\n",
        "Consider other models: https://huggingface.co/spaces/mteb/leaderboard\n",
        "https://huggingface.co/intfloat/multilingual-e5-large-instruct - very lightweight but great performance\n",
        "'intfloat/multilingual-e5-large-instruct'\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Prefix prompt (required for E5 instruct models)\n",
        "sentence = \"passage: I love nature and I'm very calm.\"\n",
        "\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Mean Pooling\n",
        "embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Test program for using AutoTokenizer instead of SentenceTransformer\n",
        "# Function to generate embedding\n",
        "def get_embedding(text, prefix=\"passage\"):\n",
        "    full_text = f\"{prefix}: {text}\"\n",
        "    inputs = tokenizer(full_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(**inputs)\n",
        "    return output.last_hidden_state.mean(dim=1)  # shape: (1, hidden_dim)\n",
        "\n",
        "# Generate Pokémon embeddings (passages)\n",
        "print(\"Embedding Pokémon descriptions...\")\n",
        "pokemon_embeddings = []\n",
        "for desc in df[\"biology\"]:\n",
        "    emb = get_embedding(desc, prefix=\"passage\")\n",
        "    pokemon_embeddings.append(emb)\n",
        "\n",
        "# Stack into one tensor\n",
        "pokemon_tensor = torch.vstack(pokemon_embeddings)  # shape: (num_pokemon, hidden_dim)\n",
        "\n",
        "# Get user input\n",
        "user_input = '\n",
        "My favorite color is baby blue. My dream job is to work remotely as a machine learning engineer while working on my DIY hobby as a side hustle.\n",
        "'\n",
        "user_emb = get_embedding(user_input, prefix=\"query\")\n",
        "\n",
        "# Compute cosine similarities\n",
        "similarities = cosine_similarity(user_emb, pokemon_tensor)[0]  # shape: (num_pokemon,)\n",
        "df[\"similarity\"] = similarities.cpu().numpy()\n",
        "\n",
        "# Sort by similarity\n",
        "top_matches = df.sort_values(by=\"similarity\", ascending=False).head(6)\n",
        "print(top_matches[[\"name\", \"similarity\"]])\n",
        "df.sort_values(by=\"similarity\", ascending=False).head(n=10)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "\n"
      ],
      "metadata": {
        "id": "CCi0Krd0KJB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c58215c9-66fb-446e-d9dc-30167f35f600"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2025-05-06 07:23:15.567180: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-06 07:23:15.734790: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "You try to use a model that was created with version 2.4.0.dev0, however, your version is 2.4.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
            "\n",
            "\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 500: named symbol not found (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-Tuning SentenceTransformer on Custom Similarity Pairs\n",
        "\n",
        "train_data = [\n",
        "    InputExample(texts=[\"calm and caring\", \"Bulbasaur is peaceful\"], label=0.9),\n",
        "    InputExample(texts=[\"fiery and competitive\", \"Charizard is aggressive\"], label=0.95),\n",
        "    InputExample(texts=[\"lazy and sleepy\", \"Snorlax loves to sleep\"], label=1.0),\n",
        "    InputExample(texts=[\"energetic and active\", \"Snorlax is lazy\"], label=0.1),\n",
        "]\n",
        "\n",
        "#train_data = SentencesDataset(train_data, model.get_tokenizer())\n",
        "\n",
        "# Train\n",
        "train_dataset = SentencesDataset(train_data, model)\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=4)\n",
        "train_loss = losses.CosineSimilarityLoss(model)\n",
        "\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3, show_progress_bar=True)\n",
        "model.save(\"custom-pokemon-matcher\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCAFWX8cn9_f",
        "outputId": "b4bdd960-e4e4-4526-9022-b0ea5cab25f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration: 100%|██████████| 1/1 [00:04<00:00,  4.89s/it]\n",
            "Epoch:  33%|███▎      | 1/3 [00:04<00:09,  4.89s/it]\n",
            "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration: 100%|██████████| 1/1 [00:03<00:00,  3.68s/it]\n",
            "Epoch:  67%|██████▋   | 2/3 [00:08<00:04,  4.18s/it]\n",
            "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration: 100%|██████████| 1/1 [00:03<00:00,  3.52s/it]\n",
            "Epoch: 100%|██████████| 3/3 [00:12<00:00,  4.04s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save pokemon embeddings\n",
        "#df = pd.read_csv(\"pokemon_data.csv\", sep='^([^,]+),')\n",
        "EMBEDDINGS_PATH = 'pokemon_embeddings.pt'\n",
        "embeddings = model.encode(list(df['biology'] + df['auxiliary']), convert_to_tensor=True)   #\n",
        "# embeddings = model.encode(df['biology'].tolist(), convert_to_tensor=True)   # torch.Size([151, 384]) for 151 pokemon\n",
        "torch.save({'names': df['name'].tolist(), 'embeddings': embeddings}, EMBEDDINGS_PATH)\n",
        "\n",
        "#user_text = \"I'm a chill dude.\"\n",
        "#user_vec = model.encode(user_text, convert_to_tensor=True)\n",
        "#pokemon_vecs = model.encode(embeddings, convert_to_tensor=True)\n",
        "#scores = util.cos_sim(user_vec, pokemon_vecs)\n",
        "#scores"
      ],
      "metadata": {
        "id": "zXGzWMponzYA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is your favorite color?\n",
        "# What is your dream job?\n",
        "# What is your favorite food?\n",
        "# What hobbies do you have?\n",
        "# How do you like to spend your free time?\n",
        "user_text = \"My favorite color is baby blue. My dream job is to work remotely as a machine learning engineer while working on my DIY hobby as a side hustle. My favorite food would probably have to be asian cuisines like Korean, Japanese, Chinese, Vietnamese and Thai food. I love meats, spicy food and clean and healthy, but stimulating and appetizing cuisines - I like to be surprised! My hobbies are tennis, reading, coding projects, learning, working out at the gym, trying new delicious food - both eating and cooking recipes - and any sort of creative endeavors like drawing, writing, crafting, etc. I like to spend my free time relaxing, playing games like Genshin Impact, playing tennis, surfing through social media and YouTube for interesting content, enjoying a good weather with good food, and such.\"\n",
        "#user_text = \"My favorite color is sage. My dream job is to be a professor. My favorite food is tteokbokki, hotpot, gopchang, and pastry. I like to bake and swim as my hobbies. In my free time, I like to hang out with my friends.\"\n",
        "user_vec = model.encode(user_text, convert_to_tensor=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5yG6r9T6faAf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### Trying running with & without text preprocessing #################\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9.,;!?()\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in text.split() if word not in stop_words]\n",
        "    text = \" \".join(tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "user_text = clean_text(user_text)"
      ],
      "metadata": {
        "id": "quVkhQwHWv2O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2521dc5d-6258-433d-a42f-f563a27c74f6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find top matches\n",
        "def find_top_matches(user_input: str, top_k: int = 6):\n",
        "    # Load model + precomputed Pokémon embeddings\n",
        "    user_embedding = model.encode(user_input, convert_to_tensor=True)\n",
        "\n",
        "    data = torch.load(EMBEDDINGS_PATH, weights_only=True)  # weights_only=False uses default pickle module implicitly, which can construct malicious pickle data that will execute arbitrary code during unpickling.\n",
        "    names = data['names']\n",
        "    pokemon_embeddings = data['embeddings']   # 'description'?\n",
        "\n",
        "    # Cosine similarity\n",
        "    cosine_scores = util.cos_sim(user_embedding, pokemon_embeddings)[0]\n",
        "\n",
        "    # Get top matches\n",
        "    top_results = torch.topk(cosine_scores, k=top_k)\n",
        "\n",
        "    results = []\n",
        "    for idx, score in zip(top_results.indices, top_results.values):\n",
        "        results.append({\n",
        "            'name': names[idx],\n",
        "            'score': round(score.item(), 4)\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "print('Top matches')\n",
        "matches = find_top_matches(user_text)\n",
        "for m in matches:\n",
        "  print(f\"{m['name']}: {m['score']*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aESQI8TQKI9x",
        "outputId": "604d9a6c-4d0f-4537-df37-7ba3648ca293"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matches\n",
            "Nidoran♀: 78.69%\n",
            "Nidoran♂: 78.49%\n",
            "Slowpoke: 78.21%\n",
            "Hitmonchan: 78.13%\n",
            "Doduo: 77.96%\n",
            "Seadra: 77.80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "knn = NearestNeighbors(n_neighbors=6, metric='cosine')\n",
        "knn.fit(embeddings)\n",
        "distances, indices = knn.kneighbors(np.array([user_vec.numpy()]))# [user_vec])\n",
        "\n",
        "print('Proximity')\n",
        "for d, i in zip(distances.flatten(), indices.flatten()):\n",
        "    print(f'{df.name.iloc[i]}: {d:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB3RyC_-SG3f",
        "outputId": "97f4bddd-f3e0-49e2-f33d-b713b5310be0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proximity\n",
            "Nidoran♀: 0.2188\n",
            "Slowpoke: 0.2200\n",
            "Nidoran♂: 0.2211\n",
            "Articuno: 0.2218\n",
            "Seaking: 0.2226\n",
            "Vaporeon: 0.2243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web crawler"
      ],
      "metadata": {
        "id": "SnVxr-wxVjjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "BASE_URL = \"https://bulbapedia.bulbagarden.net\"\n",
        "START_URL = \"/wiki/Bulbasaur_(Pok%C3%A9mon)\"\n",
        "# \"User-Agent\" - a string your browser (or bot) sends to a server to identify itself\n",
        "# \"Mozilla/5.0\" - pretends to be a regular browser\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}   # alternatives: \"Chrome/91.0\", \"Safari/537.36\"\n",
        "\n",
        "def clean_text(text):\n",
        "    return ' '.join(text.strip().replace('\\n', ' ').split())\n",
        "\n",
        "def get_pokemon_data(page_url):\n",
        "    url = BASE_URL + page_url\n",
        "    print(f\"Scraping: {url}\")\n",
        "    res = requests.get(url, headers=HEADERS)\n",
        "    # res.text - raw HTML string returned from the webpage\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")   # converts that HTML string into a navigable object tree\n",
        "\n",
        "    data = {}\n",
        "\n",
        "    # Hyperlink\n",
        "    data[\"url\"] = url\n",
        "\n",
        "    # Name\n",
        "    # .strip() - removes whitespace in front & back\n",
        "    data[\"name\"] = soup.find(\"h1\", {\"id\": \"firstHeading\"}).text.strip()[:-10]   # Slice off ' (Pokémon)'\n",
        "\n",
        "    # Biology\n",
        "    bio_heading = soup.find(\"span\", id=\"Biology\")\n",
        "    if bio_heading:\n",
        "        # find_parent() - finds the closest ancestor element (parent tag) of the current element\n",
        "        # find_next_sibling(\"p\") - finds the next sibling tag (same level in the DOM) of type <p> after the current element\n",
        "        biology = bio_heading.find_parent().find_next_sibling(\"p\")   # find the first ancestor which name is p\n",
        "        data[\"biology\"] = clean_text(biology.text) if biology else \"\"\n",
        "\n",
        "    # Type and Abilities\n",
        "    infobox = soup.find(\"table\", class_=\"roundy\")\n",
        "    if infobox:\n",
        "        # title=lambda x: x and \"Type\" in x checks that the title attribute exists and contains \"Type\"\n",
        "        type_row = infobox.find(\"a\", title=lambda x: x and \"Type\" in x)\n",
        "        if type_row:\n",
        "            # find_all_next() - finds all matching elements that appear after the current tag (not just siblings—anywhere forward)\n",
        "            types = type_row.find_all_next(\"a\", title=lambda x: x and \"(type)\" in x)\n",
        "            data[\"types\"] = [t.text for t in types[:2]]  # max 2 types\n",
        "\n",
        "        abilities = infobox.find_all(\"a\", title=lambda x: x and \"Ability\" in x)\n",
        "        data[\"abilities\"] = list(set([a.text for a in abilities]))\n",
        "\n",
        "        # Leveling rate\n",
        "        leveling_rate = \"\"\n",
        "        try:\n",
        "            exp_td_all = soup.find_all(\"td\", class_=\"roundy\")\n",
        "            for exp_td in exp_td_all:\n",
        "                # Find the correct 'td' with experience table\n",
        "                if exp_td and exp_td.find(\"a\", title=\"Experience\"):\n",
        "                    leveling_rate = exp_td.find(\"table\", class_=\"roundy\").find(\n",
        "                              \"tbody\").find(\"tr\").find(\"td\").text.strip()\n",
        "                    \"\"\"inner_table = exp_td.find(\"table\", class_=\"roundy\")\n",
        "                    if inner_table:\n",
        "                        cell = inner_table.find(\"tbody\").find(\"tr\").find(\"td\")\n",
        "                        if cell:\n",
        "                            leveling_rate = cell.text.strip()\"\"\"\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Failed to extract leveling rate for {data['name']}: {e}\")\n",
        "\n",
        "        data[\"leveling_rate\"] = leveling_rate\n",
        "\n",
        "    # Link to next Pokémon\n",
        "    next_link = None\n",
        "    for a_tag in soup.find_all(\"a\", href=True):\n",
        "        # Find link with href containing \"(Pok%C3%A9mon)\" and child span with → arrow\n",
        "        if \"(Pok%C3%A9mon)\" in a_tag['href']:\n",
        "            span = a_tag.find(\"span\", style=\"color:#000;\")\n",
        "            if span and \"→\" in span.text:\n",
        "                next_link = a_tag['href']\n",
        "                break\n",
        "\n",
        "    data[\"next_link\"] = next_link\n",
        "\n",
        "    return data\n",
        "\n",
        "def crawl_pokemon(limit=10):\n",
        "    all_data = []\n",
        "    current_url = START_URL\n",
        "    visited = set()\n",
        "\n",
        "    for _ in range(limit):\n",
        "        if current_url in visited:\n",
        "            break\n",
        "        visited.add(current_url)\n",
        "\n",
        "        try:\n",
        "            data = get_pokemon_data(current_url)\n",
        "            all_data.append(data)\n",
        "            current_url = data.get(\"next_link\")\n",
        "            # https://bulbapedia.bulbagarden.net/robots.txt crawl-delay 5\n",
        "            time.sleep(5)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error: {e}\")\n",
        "            break\n",
        "\n",
        "        if not current_url:\n",
        "            break\n",
        "\n",
        "    return all_data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pokemon_data = crawl_pokemon(limit=1)\n",
        "    df = pd.DataFrame(pokemon_data)\n",
        "    df.to_csv(\"test_pokemon_data.csv\", index=False)   ############# change to pokemon_data.csv\n",
        "    print(\"✅ Saved pokemon_data.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNSGz3h7i_AW",
        "outputId": "538ee9aa-ff31-4ffb-880d-2a82769344f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: https://bulbapedia.bulbagarden.net/wiki/Bulbasaur_(Pok%C3%A9mon)\n",
            "Grass\n",
            "<class 'str'>\n",
            "[<a href=\"/wiki/Ability\" title=\"Ability\"><span style=\"color:#000;\">Abilities</span></a>, <a href=\"/wiki/Overgrow_(Ability)\" title=\"Overgrow (Ability)\"><span style=\"color:#000;\">Overgrow</span></a>, <a href=\"/wiki/Cacophony_(Ability)\" title=\"Cacophony (Ability)\"><span style=\"color:#000;\">Cacophony</span></a>, <a href=\"/wiki/Cacophony_(Ability)\" title=\"Cacophony (Ability)\"><span style=\"color:#000;\">Cacophony</span></a>, <a href=\"/wiki/Chlorophyll_(Ability)\" title=\"Chlorophyll (Ability)\"><span style=\"color:#000;\">Chlorophyll</span></a>, <a href=\"/wiki/Cacophony_(Ability)\" title=\"Cacophony (Ability)\"><span style=\"color:#000;\">Cacophony</span></a>, <a href=\"/wiki/Cacophony_(Ability)\" title=\"Cacophony (Ability)\"><span style=\"color:#000;\">Cacophony</span></a>, <a href=\"/wiki/Cacophony_(Ability)\" title=\"Cacophony (Ability)\"><span style=\"color:#000;\">Cacophony</span></a>]\n",
            "<class 'bs4.element.ResultSet'>\n",
            "✅ Saved pokemon_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "poN_OKZJWmV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "# Remove (Pokemon) from name\n",
        "df = pd.read_csv('pokemon_data.csv')\n",
        "\n",
        "# Convert string containing literal to list\n",
        "df['types'] = df['types'].apply(lambda x: ast.literal_eval(x))\n",
        "df['abilities'] = df['abilities'].apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "for i in range(len(df)):\n",
        "    # Remove ' (Pokemon)' from 'name'\n",
        "    df['name'][i] = df.name.iloc[i][:-10]\n",
        "    # Remove \"Abilities\" from \"abilities\"\n",
        "    if \"Abilities\" in df['abilities'][i]:\n",
        "        df['abilities'][i].remove(\"Abilities\")\n",
        "\n",
        "# Synthesize auxiliary data using other columns\n",
        "df['auxiliary'] = df.apply(lambda x:\n",
        "                f\"{x['name']} is a Pokémon of type {x['types']}, with main abilities {x['abilities']} and leveling rate of {x['leveling_rate']}.\", axis=1)\n",
        "\n",
        "\n",
        "\n",
        "df.head(n=3)\n"
      ],
      "metadata": {
        "id": "W79oKJ1fi-6k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "ba0acb57-795b-4188-a453-70da5392b7c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 url       name  \\\n",
              "0  https://bulbapedia.bulbagarden.net/wiki/Bulbas...  Bulbasaur   \n",
              "1  https://bulbapedia.bulbagarden.net/wiki/Ivysau...    Ivysaur   \n",
              "2  https://bulbapedia.bulbagarden.net/wiki/Venusa...   Venusaur   \n",
              "\n",
              "                                             biology            types  \\\n",
              "0  Bulbasaur is a small, quadrupedal amphibian Po...  [Grass, Poison]   \n",
              "1  Ivysaur is a quadrupedal amphibian Pokémon tha...  [Grass, Poison]   \n",
              "2  Venusaur is a squat, quadrupedal amphibian Pok...  [Grass, Poison]   \n",
              "\n",
              "                                       abilities leveling_rate  \\\n",
              "0             [Overgrow, Cacophony, Chlorophyll]   Medium Slow   \n",
              "1             [Overgrow, Cacophony, Chlorophyll]   Medium Slow   \n",
              "2  [Overgrow, Cacophony, Thick Fat, Chlorophyll]   Medium Slow   \n",
              "\n",
              "                         next_link  \\\n",
              "0     /wiki/Ivysaur_(Pok%C3%A9mon)   \n",
              "1    /wiki/Venusaur_(Pok%C3%A9mon)   \n",
              "2  /wiki/Charmander_(Pok%C3%A9mon)   \n",
              "\n",
              "                                           auxiliary  \n",
              "0  Bulbasaur is a Pokémon of type ['Grass', 'Pois...  \n",
              "1  Ivysaur is a Pokémon of type ['Grass', 'Poison...  \n",
              "2  Venusaur is a Pokémon of type ['Grass', 'Poiso...  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>name</th>\n",
              "      <th>biology</th>\n",
              "      <th>types</th>\n",
              "      <th>abilities</th>\n",
              "      <th>leveling_rate</th>\n",
              "      <th>next_link</th>\n",
              "      <th>auxiliary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://bulbapedia.bulbagarden.net/wiki/Bulbas...</td>\n",
              "      <td>Bulbasaur</td>\n",
              "      <td>Bulbasaur is a small, quadrupedal amphibian Po...</td>\n",
              "      <td>[Grass, Poison]</td>\n",
              "      <td>[Overgrow, Cacophony, Chlorophyll]</td>\n",
              "      <td>Medium Slow</td>\n",
              "      <td>/wiki/Ivysaur_(Pok%C3%A9mon)</td>\n",
              "      <td>Bulbasaur is a Pokémon of type ['Grass', 'Pois...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://bulbapedia.bulbagarden.net/wiki/Ivysau...</td>\n",
              "      <td>Ivysaur</td>\n",
              "      <td>Ivysaur is a quadrupedal amphibian Pokémon tha...</td>\n",
              "      <td>[Grass, Poison]</td>\n",
              "      <td>[Overgrow, Cacophony, Chlorophyll]</td>\n",
              "      <td>Medium Slow</td>\n",
              "      <td>/wiki/Venusaur_(Pok%C3%A9mon)</td>\n",
              "      <td>Ivysaur is a Pokémon of type ['Grass', 'Poison...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://bulbapedia.bulbagarden.net/wiki/Venusa...</td>\n",
              "      <td>Venusaur</td>\n",
              "      <td>Venusaur is a squat, quadrupedal amphibian Pok...</td>\n",
              "      <td>[Grass, Poison]</td>\n",
              "      <td>[Overgrow, Cacophony, Thick Fat, Chlorophyll]</td>\n",
              "      <td>Medium Slow</td>\n",
              "      <td>/wiki/Charmander_(Pok%C3%A9mon)</td>\n",
              "      <td>Venusaur is a Pokémon of type ['Grass', 'Poiso...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9DZSEUkgRwRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-0Zdb8jSGs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jBa-V4eH4FSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Webscrape pokedex's pokemon biology description (and some numerical data)\n",
        "# Use\n",
        "# Things to try 20250504:\n",
        "  # try it with auxiliary data on embeddings & compare performance\n",
        "  # try w & w/o text preprocessing & compare performance\n",
        "\n",
        "  # cosine similarity pairs\n",
        "  # new models: all-mpnet-base-v2 -> instruct\n",
        "\n"
      ],
      "metadata": {
        "id": "y8NQI3-V4FNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz4jQTjZ3_4A"
      },
      "outputs": [],
      "source": []
    }
  ]
}